import numpy as np
import copy
import heapq
import time
from collections import defaultdict

# ==========================================
# I. ç»Ÿä¸€æ¯”èµ›ç¯å¢ƒ (RoboconEnvironment)
# ==========================================

class RoboconEnvironment:
    """
    ç»Ÿä¸€çš„æ¯”èµ›ç¯å¢ƒã€‚ç”¨äº AlphaBetaBot å’Œ AStarBot ä¹‹é—´çš„æ•°æ®äº¤äº’ã€‚
    ç©å®¶ 1 (Max/A* Bot) ä½¿ç”¨ +1ï¼Œç©å®¶ -1 (Min/AlphaBeta Bot) ä½¿ç”¨ -1ã€‚
    """
    def __init__(self, initial_board=None, initial_r1=5, initial_r2=5, initial_weapons=3, time_limit=180.0):
        # æ£‹ç›˜ç»“æ„: 3x3x2 æ•°ç»„ (Owner/Type)
        # board[r, c, 0] = Owner (1:P1, -1:P2, 0:Empty)
        # board[r, c, 1] = Type (1:R1-KFS, 2:R2-KFS, 0:Empty)
        if initial_board is None:
            self.board = np.zeros((3, 3, 2), dtype=int)
        else:
            self.board = initial_board.copy()
            
        # èµ„æº
        self.r1_kfs = {1: initial_r1, -1: initial_r1}
        self.r2_kfs = {1: initial_r2, -1: initial_r2}
        self.weapons = {1: initial_weapons, -1: initial_weapons}
        
        # æˆæœ¬/æ—¶é—´
        self.costs = {1: 0.0, -1: 0.0}
        self.time_limit = time_limit
        
        # å½“å‰ç©å®¶
        self.current_player = 1
        self.game_over = False
        self.winner = 0  # 1: P1 win, -1: P2 win, 0: Draw
        
        # åŠ¨ä½œæ—¶é—´ (ç®€åŒ–Roboconæ¨¡å‹)
        self.T_PLACE = {
            0: 8.0,  # é¡¶å±‚
            1: 5.0,  # ä¸­å±‚
            2: 5.0   # åº•å±‚
        }
        self.T_ATTACK = 6.0
        self.P_FAIL_PLACE = 0.1
        self.P_FAIL_ATTACK = 0.2

    def clone(self):
        new_env = RoboconEnvironment(
            initial_board=self.board, 
            initial_r1=self.r1_kfs[1], 
            initial_r2=self.r2_kfs[1], 
            initial_weapons=self.weapons[1],
            time_limit=self.time_limit
        )
        new_env.r1_kfs = self.r1_kfs.copy()
        new_env.r2_kfs = self.r2_kfs.copy()
        new_env.weapons = self.weapons.copy()
        new_env.costs = self.costs.copy()
        new_env.current_player = self.current_player
        new_env.game_over = self.game_over
        new_env.winner = self.winner
        return new_env

    def _get_cost(self, action_type, pos=None):
        """è®¡ç®—åŠ¨ä½œçš„é£é™©åŠ æƒæˆæœ¬"""
        if action_type == 'place':
            r = pos[0]
            cost_time = self.T_PLACE.get(r, 5.0)
            return cost_time / (1.0 - self.P_FAIL_PLACE)
        elif action_type == 'attack':
            return self.T_ATTACK / (1.0 - self.P_FAIL_ATTACK)
        return 0.0

    def get_valid_actions(self):
        actions = []
        if self.game_over or self.costs[self.current_player] >= self.time_limit:
            return actions

        player = self.current_player
        
        # --- æ”¾ç½®åŠ¨ä½œ ---
        for r in range(3):
            for c in range(3):
                if self.board[r, c, 0] == 0:
                    pos = (r, c)
                    # æ”¾ç½® R1-KFS (Type 1)
                    if self.r1_kfs[player] > 0:
                        actions.append({
                            'type': 'place', 'kfs_type': 1, 'pos': pos, 'player': player
                        })
                    # æ”¾ç½® R2-KFS (Type 2)
                    if self.r2_kfs[player] > 0:
                        actions.append({
                            'type': 'place', 'kfs_type': 2, 'pos': pos, 'player': player
                        })
        
        # --- æ”»å‡»åŠ¨ä½œ ---
        if self.weapons[player] > 0:
            for r in range(3):
                for c in range(3):
                    # æ”»å‡»å¯¹æ–¹çš„æ£‹å­
                    if self.board[r, c, 0] == -player:
                        pos = (r, c)
                        actions.append({
                            'type': 'attack', 'pos': pos, 'player': player
                        })
        
        return actions

    def step(self, action):
        """æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œ"""
        action_type = action['type']
        pos = action['pos']
        player = action['player']
        
        cost = self._get_cost(action_type, pos)
        
        if action_type == 'place':
            kfs_type = action['kfs_type']
            self.board[pos[0], pos[1], 0] = player
            self.board[pos[0], pos[1], 1] = kfs_type
            if kfs_type == 1:
                self.r1_kfs[player] -= 1
            else:
                self.r2_kfs[player] -= 1
        
        elif action_type == 'attack':
            self.board[pos[0], pos[1], 0] = 0
            self.board[pos[0], pos[1], 1] = 0
            self.weapons[player] -= 1
        
        self.costs[player] += cost
        
        # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
        self._check_game_over()
        
        # åˆ‡æ¢ç©å®¶
        if not self.game_over:
            self.current_player = -self.current_player

    def _get_score(self, player):
        """è®¡ç®—æŒ‡å®šç©å®¶çš„å¾—åˆ†"""
        score = 0
        for r in range(3):
            for c in range(3):
                if self.board[r, c, 0] == player:
                    kfs_type = self.board[r, c, 1]
                    
                    # R1 (type 1): 40/60/90; R2 (type 2): 50/70/100
                    if r == 0:  # é¡¶å±‚åŒºåŸŸ
                        score += 90 if kfs_type == 1 else 100
                    elif r == 1:  # ä¸­å±‚åŒºåŸŸ
                        score += 60 if kfs_type == 1 else 70
                    elif r == 2:  # åº•å±‚åŒºåŸŸ
                        score += 40 if kfs_type == 1 else 50
        return score
    
    def _check_grand_master(self, player):
        """æ£€æŸ¥æŒ‡å®šç©å®¶æ˜¯å¦è¾¾æˆç»å¯¹èƒœåˆ© (ä¸‰è¿ä¸”å«R2)"""
        
        # åˆ›å»ºä¸€ä¸ªåªåŒ…å« Owner çš„ 3x3 çŸ©é˜µ
        owner_board = self.board[:, :, 0]
        
        # æ£€æŸ¥æ‰€æœ‰è¡Œ/åˆ—/å¯¹è§’çº¿
        for i in range(3):
            # æ£€æŸ¥åˆ—
            if np.all(owner_board[:, i] == player):
                types = self.board[:, i, 1]
                if 2 in types: 
                    return True
            # æ£€æŸ¥è¡Œ
            if np.all(owner_board[i, :] == player):
                types = self.board[i, :, 1]
                if 2 in types: 
                    return True
        
        # æ£€æŸ¥å¯¹è§’çº¿
        diag1_owner = owner_board.diagonal()
        diag2_owner = np.fliplr(owner_board).diagonal()
        
        if np.all(diag1_owner == player):
            types = self.board.diagonal()[:, 1]
            if 2 in types: 
                return True
            
        if np.all(diag2_owner == player):
            # ç›´æ¥ä»åŸå§‹ board ä¸­æŠ½å–åå‘å¯¹è§’çº¿ç±»å‹
            types = np.array([self.board[i, 2-i, 1] for i in range(3)]) 
            if 2 in types: 
                return True
            
        return False
    
    def _check_game_over(self):
        """æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ"""
        
        # 1. ç»å¯¹èƒœåˆ©æ£€æŸ¥ (Robocon ç»ˆå±€è§„åˆ™)
        if self._check_grand_master(1):
            self.game_over = True
            self.winner = 1
            return
        if self._check_grand_master(-1):
            self.game_over = True
            self.winner = -1
            return

        # 2. è€—å°½èµ„æºæˆ–æ—¶é—´æ£€æŸ¥
        actions_p1 = self.get_valid_actions_no_time_check(1)
        actions_p2 = self.get_valid_actions_no_time_check(-1)

        # æ£‹ç›˜æ»¡ æˆ– åŒæ–¹è€—å°½èµ„æº/æ—¶é—´
        if (len(actions_p1) == 0 and len(actions_p2) == 0) or \
           (self.costs[1] >= self.time_limit and self.costs[-1] >= self.time_limit):
            self.game_over = True
            score_p1 = self._get_score(1)
            score_p2 = self._get_score(-1)
            
            if score_p1 > score_p2:
                self.winner = 1
            elif score_p2 > score_p1:
                self.winner = -1
            else:
                self.winner = 0  # å¹³å±€
            return
    
    def get_valid_actions_no_time_check(self, player):
        """ç”¨äºåˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸæ—¶ï¼Œæ£€æŸ¥èµ„æºæ˜¯å¦è€—å°½ã€‚è¿”å›å®é™…çš„åŠ¨ä½œåˆ—è¡¨ã€‚"""
        actions = []
        # --- æ”¾ç½®åŠ¨ä½œ ---
        for r in range(3):
            for c in range(3):
                if self.board[r, c, 0] == 0:
                    pos = (r, c)
                    if self.r1_kfs[player] > 0: 
                        actions.append({'type': 'place', 'kfs_type': 1, 'pos': pos, 'player': player})
                    if self.r2_kfs[player] > 0: 
                        actions.append({'type': 'place', 'kfs_type': 2, 'pos': pos, 'player': player})
        
        # --- æ”»å‡»åŠ¨ä½œ ---
        if self.weapons[player] > 0:
            for r in range(3):
                for c in range(3):
                    if self.board[r, c, 0] == -player:
                        actions.append({'type': 'attack', 'pos': (r, c), 'player': player})

        return actions

# ==========================================
# II. AlphaBeta Bot (Player -1)
# ==========================================

class AlphaBetaBot:
    def __init__(self, depth=3):
        self.search_depth = depth

    def get_best_move(self, env):
        actions = env.get_valid_actions()
        if not actions:
            return None

        best_val = float('inf')  # AlphaBetaBot æ‰®æ¼” Player -1 (Min)
        best_action = None

        # æ ¹èŠ‚ç‚¹éå†
        is_maximizing = True  # å› ä¸ºä¸‹ä¸€æ­¥æ˜¯å¯¹æ–¹(P1)èµ°

        for action in actions:
            next_env = env.clone()
            next_env.step(action)
            
            # é€’å½’æœç´¢
            val = self._alpha_beta(next_env, self.search_depth - 1, -float('inf'), float('inf'), is_maximizing)
            
            if val < best_val:
                best_val = val
                best_action = action

        return best_action

    def _alpha_beta(self, env, depth, alpha, beta, is_maximizing):
        # --- 1. ç»ˆæ­¢æ¡ä»¶ ---
        if depth == 0 or env.game_over:
            return self._evaluate(env)

        actions = env.get_valid_actions()
        if not actions: 
            return self._evaluate(env)

        # --- 2. é€’å½’æœç´¢ ---
        if is_maximizing:  # Player 1 (Max)
            max_eval = -float('inf')
            for action in actions:
                next_env = env.clone()
                next_env.step(action)
                eval_val = self._alpha_beta(next_env, depth - 1, alpha, beta, False)
                max_eval = max(max_eval, eval_val)
                alpha = max(alpha, eval_val)
                if beta <= alpha:
                    break
            return max_eval
        else:  # Player -1 (Min)
            min_eval = float('inf')
            for action in actions:
                next_env = env.clone()
                next_env.step(action)
                eval_val = self._alpha_beta(next_env, depth - 1, alpha, beta, True)
                min_eval = min(min_eval, eval_val)
                beta = min(beta, eval_val)
                if beta <= alpha:
                    break
            return min_eval

    def _evaluate(self, env):
        """ é™æ€è¯„ä¼°å‡½æ•°ï¼šæ­£åˆ† -> P1 æœ‰åˆ©ï¼›è´Ÿåˆ† -> P2 (-1) æœ‰åˆ© """
        P1 = 1
        P2 = -1
        score = 0.0

        # --- A. ç»ˆå±€è¯„åˆ† (å·¨å¤§å¥–åŠ±/æƒ©ç½š) ---
        if env.game_over:
            if env.winner == P1:
                return 1000000.0 - env.costs[P1]
            elif env.winner == P2:
                return -1000000.0 + env.costs[P2]
            else:  # å¹³å±€
                return env._get_score(P1) - env._get_score(P2) 

        # --- B. è¿‡ç¨‹è¯„åˆ† ---
        
        # 1. å®é™…å¾—åˆ†ä¼˜åŠ¿
        score += 1.0 * (env._get_score(P1) - env._get_score(P2))

        # 2. ç»å¯¹èƒœåˆ©å¨èƒ
        owner_board = env.board[:, :, 0]
        lines = []
        lines.extend([owner_board[:, i] for i in range(3)])  # åˆ—
        lines.extend([owner_board[i, :] for i in range(3)])  # è¡Œ
        lines.append(owner_board.diagonal())                 # å¯¹è§’1
        lines.append(np.fliplr(owner_board).diagonal())      # å¯¹è§’2

        for line in lines:
            cnt_p1 = np.sum(line == P1)
            cnt_p2 = np.sum(line == P2)

            if cnt_p2 == 0:
                if cnt_p1 == 2: 
                    score += 500.0  # P1 å·®ä¸€æ­¥è¿çº¿
                elif cnt_p1 == 1: 
                    score += 50.0

            if cnt_p1 == 0:
                if cnt_p2 == 2: 
                    score -= 500.0  # P2 å·®ä¸€æ­¥è¿çº¿
                elif cnt_p2 == 1: 
                    score -= 50.0

        # 3. èµ„æºä¼˜åŠ¿
        score += 10.0 * (env.r2_kfs[P1] - env.r2_kfs[P2])
        score += 5.0 * (env.weapons[P1] - env.weapons[P2])
        
        # 4. æ—¶é—´åŠ£åŠ¿
        score += (env.costs[P2] - env.costs[P1]) * 0.1

        return score

# ==========================================
# III. A* Bot (Player +1)
# ==========================================

class AStarBot:
    """
    åŸºäºæ··åˆç›®æ ‡ A* çš„å•æ­¥å†³ç­–è€…
    """
    def __init__(self):
        # A* æ ¸å¿ƒå‚æ•°
        self.P_WIN = 1000.0  # ç»å¯¹èƒœåˆ©çš„å·¨å¤§æƒ©ç½šé¡¹
        self.W_SCORE = 1.0   # å¾—åˆ†åç½®æƒé‡

    def get_best_move(self, env):
        actions = env.get_valid_actions()
        if not actions:
            return None

        best_f_mixed = float('inf')  # A* Bot (P1) å¯»æ±‚æœ€å°çš„ F_mixed
        best_action = None
        
        for action in actions:
            next_env = env.clone()
            next_env.step(action)
            
            # è®¡ç®— G(n) å’Œ H(n)
            temp_state = self._convert_env_to_state(next_env)
            h_mixed = self._heuristic(temp_state, env)
            f_mixed = next_env.costs[1] + h_mixed
            
            if f_mixed < best_f_mixed:
                best_f_mixed = f_mixed
                best_action = action

        return best_action

    def _convert_env_to_state(self, env):
        """å°† numpy ç¯å¢ƒè½¬æ¢ä¸º A* å‹å¥½çš„åˆ—è¡¨ç»“æ„"""
        board_list = []
        for r in range(3):
            row = []
            for c in range(3):
                owner = env.board[r, c, 0]
                kfs_type = env.board[r, c, 1]
                row.append({'owner': owner, 'type': kfs_type})
            board_list.append(row)
            
        return GameStateDummy(
            board=board_list,
            r1_kfs=env.r1_kfs[1],
            r2_kfs=env.r2_kfs[1],
            weapons=env.weapons[1]
        )

    def _calculate_h_gm(self, state, env):
        """è®¡ç®— H_GM: é¢„ä¼°è¾¾æˆç»å¯¹èƒœåˆ©æ‰€éœ€çš„æœ€å°æ—¶é—´ (åªçœ‹ P1 è§†è§’)"""
        if env._check_grand_master(1): 
            return 0, 0 

        min_steps_needed = 9 
        
        # æ£€æŸ¥æ‰€æœ‰è¿çº¿
        indices = []
        for c in range(3): 
            indices.append([(r, c) for r in range(3)])  # åˆ—
        for r in range(3): 
            indices.append([(r, c) for c in range(3)])  # è¡Œ
        indices.append([(i, i) for i in range(3)])
        indices.append([(i, 2-i) for i in range(3)])
        
        for line_idx in indices:
            my_count, enemy_count, r2_present = 0, 0, False
            for r, c in line_idx:
                cell = state.board[r][c]
                if cell['owner'] == 1: 
                    my_count += 1
                    if cell['type'] == 2: 
                        r2_present = True
                elif cell['owner'] == -1: 
                    enemy_count += 1
            
            steps = 0
            # æ”»å‡»æ•Œäºº
            if enemy_count > 0:
                if state.weapons >= enemy_count: 
                    steps += enemy_count * env._get_cost('attack')
                else: 
                    continue  # æ— æ³•æ¸…ç†éšœç¢
            
            # æ”¾ç½®æ‰€éœ€å­
            needed_placements = 3 - my_count - enemy_count
            if needed_placements < 0: 
                needed_placements = 0
            
            # å‡è®¾æ‰€æœ‰æ”¾ç½®éƒ½å‘ç”Ÿåœ¨ä»£ä»·æœ€å°çš„åº•å±‚ (r=2)
            steps += needed_placements * env.T_PLACE[2] 
            
            # æ£€æŸ¥ R2-KFS èµ„æºæ˜¯å¦è¶³å¤Ÿ
            r2_needed = 0
            if needed_placements > 0 and not r2_present:
                # è‡³å°‘éœ€è¦ä¸€ä¸ª R2
                r2_needed = 1 
            
            if r2_needed > state.r2_kfs:
                continue  # R2 èµ„æºä¸è¶³
                
            # è®¡ç®—æ€»æ­¥æ•°ï¼ˆç®€åŒ–ä¸ºæ—¶é—´ï¼‰
            total_time = steps
            if total_time < min_steps_needed:
                min_steps_needed = total_time
                
        H_GM = min_steps_needed 
        return H_GM, min_steps_needed / env.T_PLACE[2] if env.T_PLACE[2] > 0 else 9

    def _heuristic(self, state, env):
        """è®¡ç®— F_mixed çš„ H(n) éƒ¨åˆ†: H_GM - ScoreBias + WinBonus"""
        H_GM, min_steps = self._calculate_h_gm(state, env)
        
        P_win = self.P_WIN
        
        # WinBonus
        if env._check_grand_master(1):
            win_bonus = -P_win  # å·²è¾¾æˆ
        elif H_GM < 3 * env.T_PLACE[2]: 
            win_bonus = -P_win + H_GM  # é¢„ä¼°å¯è¾¾æˆ
        else:
            win_bonus = 0.0
            
        # ScoreBias
        score_bias = self.W_SCORE * env._get_score(1)
        
        return H_GM - score_bias + win_bonus

# --- A* å†…éƒ¨ä½¿ç”¨çš„ç®€åŒ–çŠ¶æ€ç±» ---
class GameStateDummy:
    def __init__(self, board, r1_kfs, r2_kfs, weapons):
        self.board = board
        self.r1_kfs = r1_kfs
        self.r2_kfs = r2_kfs
        self.weapons = weapons

# ==========================================
# IV. è§†è§’ç¿»è½¬å‡½æ•°
# ==========================================

def flip_perspective(env):
    """
    åˆ›å»ºä¸€ä¸ªç¿»è½¬è§†è§’çš„ç¯å¢ƒå‰¯æœ¬ã€‚
    ç”¨äºè®©é»˜è®¤åªæ‡‚ Player 1 é€»è¾‘çš„ Bot (å¦‚ AStar) èƒ½æ‰®æ¼” Player -1ã€‚
    """
    flipped = env.clone()
    # ç¿»è½¬æ£‹ç›˜
    flipped.board[:, :, 0] = -env.board[:, :, 0]
    # ç¿»è½¬èµ„æº
    flipped.weapons = {1: env.weapons[-1], -1: env.weapons[1]}
    flipped.r1_kfs = {1: env.r1_kfs[-1], -1: env.r1_kfs[1]}
    flipped.r2_kfs = {1: env.r2_kfs[-1], -1: env.r2_kfs[1]}
    # ç¿»è½¬æˆæœ¬
    flipped.costs = {1: env.costs[-1], -1: env.costs[1]}
    flipped.current_player = -env.current_player
    return flipped

# ==========================================
# V. å•æ¬¡å¯¹æŠ—å‡½æ•°
# ==========================================

def run_single_duel(a_star_first=True):
    """
    è¿è¡Œå•æ¬¡å¯¹æŠ—
    a_star_first: Trueè¡¨ç¤ºA* Botå…ˆæ‰‹(P1), Falseè¡¨ç¤ºAlphaBeta Botå…ˆæ‰‹(P1)
    """
    # åˆå§‹åŒ–ç¯å¢ƒ - ç©ºæ£‹ç›˜ï¼Œæ— KFS
    env = RoboconEnvironment()
    
    # åˆ›å»ºä¸¤ä¸ªAI
    ab_bot = AlphaBetaBot(depth=3)
    a_star_bot = AStarBot()
    
    # æ ¹æ®å…ˆæ‰‹é¡ºåºåˆ†é…AI
    if a_star_first:
        # æƒ…æ™¯1: A*ä½œä¸ºP1å…ˆæ‰‹ï¼ŒAlphaBetaä½œä¸ºP2
        p1_bot = a_star_bot
        p2_bot = ab_bot
    else:
        # æƒ…æ™¯2: AlphaBetaä½œä¸ºP1å…ˆæ‰‹ï¼ŒA*ä½œä¸ºP2
        p1_bot = ab_bot
        p2_bot = a_star_bot
    
    no_action_count = 0
    
    while not env.game_over:
        if env.current_player == 1:
            action = p1_bot.get_best_move(env)
        else:  # Player -1
            # æ³¨æ„ï¼šA* Botéœ€è¦è§†è§’ç¿»è½¬
            if p2_bot == a_star_bot:
                flipped = flip_perspective(env)
                action = a_star_bot.get_best_move(flipped)
                # ç¿»è½¬å›åŸå§‹è§†è§’
                if action:
                    action['player'] = -1  # ç¡®ä¿ç©å®¶IDæ­£ç¡®
            else:
                action = ab_bot.get_best_move(env)
        
        if action is None:
            no_action_count += 1
            
            # æ£€æŸ¥åŒæ–¹æ˜¯å¦éƒ½æ— åŠ¨ä½œ
            actions_p1 = env.get_valid_actions_no_time_check(1)
            actions_p2 = env.get_valid_actions_no_time_check(-1)
            
            p1_no_valid_moves = (len(actions_p1) == 0 or env.costs[1] >= env.time_limit)
            p2_no_valid_moves = (len(actions_p2) == 0 or env.costs[-1] >= env.time_limit)
            
            if p1_no_valid_moves and p2_no_valid_moves:
                env._get_game_end_score_winner()
                break 
            
            # åˆ‡æ¢ç©å®¶
            env.current_player = -env.current_player
            
            if no_action_count > 2: 
                env._get_game_end_score_winner()
                break
            
            continue
        
        # æ‰§è¡ŒåŠ¨ä½œ
        env.step(action)
        no_action_count = 0
    
    # è¿”å›èƒœè€…ä¿¡æ¯
    winner = env.winner
    
    # æ ¹æ®å…ˆæ‰‹é¡ºåºåˆ¤æ–­å“ªä¸ªç®—æ³•èµ¢äº†
    if winner == 0:
        return "å¹³å±€"
    elif a_star_first:
        # A*å…ˆæ‰‹çš„æƒ…å†µ
        if winner == 1:
            return "A*èƒœ"  # A*ä½œä¸ºP1èµ¢äº†
        else:
            return "AlphaBetaèƒœ"  # AlphaBetaä½œä¸ºP2èµ¢äº†
    else:
        # AlphaBetaå…ˆæ‰‹çš„æƒ…å†µ
        if winner == 1:
            return "AlphaBetaèƒœ"  # AlphaBetaä½œä¸ºP1èµ¢äº†
        else:
            return "A*èƒœ"  # A*ä½œä¸ºP2èµ¢äº†

# ==========================================
# VI. æ‰¹é‡å¯¹æŠ—ç»Ÿè®¡
# ==========================================

def run_batch_duels(num_trials=50):
    """
    è¿è¡Œæ‰¹é‡å¯¹æŠ—ï¼Œåˆ†åˆ«ç»Ÿè®¡ä¸¤ç§å…ˆæ‰‹é¡ºåºçš„èƒœç‡
    """
    print("=" * 70)
    print("Robocon AI å¯¹æŠ—æµ‹è¯• - 50æ¬¡é‡å¤å¯¹æŠ—")
    print("åˆå§‹çŠ¶æ€: ç©ºæ£‹ç›˜ï¼Œæ— KFS")
    print("èµ„æº: R1-KFS=5, R2-KFS=5, æ­¦å™¨=3, æ—¶é—´é™åˆ¶=180ç§’")
    print("=" * 70)
    
    # æƒ…æ™¯1: A*å…ˆæ‰‹ï¼ŒAlphaBetaåæ‰‹
    print("\næƒ…æ™¯1: A* Bot å…ˆæ‰‹ (P1), AlphaBeta Bot åæ‰‹ (P2)")
    print("-" * 50)
    
    results1 = {"A*èƒœ": 0, "AlphaBetaèƒœ": 0, "å¹³å±€": 0}
    start_time1 = time.time()
    
    for i in range(num_trials):
        result = run_single_duel(a_star_first=True)
        results1[result] += 1
        
        if (i + 1) % 10 == 0:
            print(f"  å·²å®Œæˆ {i + 1}/{num_trials} æ¬¡å¯¹æŠ—...")
    
    end_time1 = time.time()
    
    # è®¡ç®—èƒœç‡
    a_star_win_rate1 = (results1["A*èƒœ"] / num_trials) * 100
    ab_win_rate1 = (results1["AlphaBetaèƒœ"] / num_trials) * 100
    draw_rate1 = (results1["å¹³å±€"] / num_trials) * 100
    
    print(f"  A*èƒœ: {results1['A*èƒœ']}æ¬¡ ({a_star_win_rate1:.2f}%)")
    print(f"  AlphaBetaèƒœ: {results1['AlphaBetaèƒœ']}æ¬¡ ({ab_win_rate1:.2f}%)")
    print(f"  å¹³å±€: {results1['å¹³å±€']}æ¬¡ ({draw_rate1:.2f}%)")
    print(f"  è€—æ—¶: {end_time1 - start_time1:.2f}ç§’")
    
    # æƒ…æ™¯2: AlphaBetaå…ˆæ‰‹ï¼ŒA*åæ‰‹
    print("\næƒ…æ™¯2: AlphaBeta Bot å…ˆæ‰‹ (P1), A* Bot åæ‰‹ (P2)")
    print("-" * 50)
    
    results2 = {"A*èƒœ": 0, "AlphaBetaèƒœ": 0, "å¹³å±€": 0}
    start_time2 = time.time()
    
    for i in range(num_trials):
        result = run_single_duel(a_star_first=False)
        results2[result] += 1
        
        if (i + 1) % 10 == 0:
            print(f"  å·²å®Œæˆ {i + 1}/{num_trials} æ¬¡å¯¹æŠ—...")
    
    end_time2 = time.time()
    
    # è®¡ç®—èƒœç‡
    a_star_win_rate2 = (results2["A*èƒœ"] / num_trials) * 100
    ab_win_rate2 = (results2["AlphaBetaèƒœ"] / num_trials) * 100
    draw_rate2 = (results2["å¹³å±€"] / num_trials) * 100
    
    print(f"  A*èƒœ: {results2['A*èƒœ']}æ¬¡ ({a_star_win_rate2:.2f}%)")
    print(f"  AlphaBetaèƒœ: {results2['AlphaBetaèƒœ']}æ¬¡ ({ab_win_rate2:.2f}%)")
    print(f"  å¹³å±€: {results2['å¹³å±€']}æ¬¡ ({draw_rate2:.2f}%)")
    print(f"  è€—æ—¶: {end_time2 - start_time2:.2f}ç§’")
    
    # ç»¼åˆç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ç»¼åˆç»Ÿè®¡ (ä¸¤ç§æƒ…æ™¯åˆå¹¶)")
    print("-" * 50)
    
    total_trials = num_trials * 2
    total_a_star_wins = results1["A*èƒœ"] + results2["A*èƒœ"]
    total_ab_wins = results1["AlphaBetaèƒœ"] + results2["AlphaBetaèƒœ"]
    total_draws = results1["å¹³å±€"] + results2["å¹³å±€"]
    
    total_a_star_win_rate = (total_a_star_wins / total_trials) * 100
    total_ab_win_rate = (total_ab_wins / total_trials) * 100
    total_draw_rate = (total_draws / total_trials) * 100
    
    print(f"æ€»å¯¹æŠ—æ¬¡æ•°: {total_trials}")
    print(f"A* Bot æ€»èƒœç‡: {total_a_star_win_rate:.2f}% ({total_a_star_wins}æ¬¡)")
    print(f"AlphaBeta Bot æ€»èƒœç‡: {total_ab_win_rate:.2f}% ({total_ab_wins}æ¬¡)")
    print(f"æ€»å¹³å±€ç‡: {total_draw_rate:.2f}% ({total_draws}æ¬¡)")
    print(f"æ€»è€—æ—¶: {end_time1 - start_time1 + end_time2 - start_time2:.2f}ç§’")
    
    # åˆ†æç»“æœ
    print("\n" + "=" * 70)
    print("ç»“æœåˆ†æ")
    print("-" * 50)
    
    if total_a_star_win_rate > total_ab_win_rate:
        print(f"âœ… A* Bot è¡¨ç°æ›´ä¼˜ (é¢†å…ˆ {total_a_star_win_rate - total_ab_win_rate:.2f}ä¸ªç™¾åˆ†ç‚¹)")
    elif total_ab_win_rate > total_a_star_win_rate:
        print(f"âœ… AlphaBeta Bot è¡¨ç°æ›´ä¼˜ (é¢†å…ˆ {total_ab_win_rate - total_a_star_win_rate:.2f}ä¸ªç™¾åˆ†ç‚¹)")
    else:
        print("ğŸ¤ åŒæ–¹è¡¨ç°ç›¸å½“")
    
    # å…ˆæ‰‹ä¼˜åŠ¿åˆ†æ
    print(f"\nå…ˆæ‰‹ä¼˜åŠ¿åˆ†æ:")
    print(f"  å…ˆæ‰‹èƒœç‡ - A*: {a_star_win_rate1:.2f}%, AlphaBeta: {ab_win_rate2:.2f}%")
    print(f"  åæ‰‹èƒœç‡ - A*: {a_star_win_rate2:.2f}%, AlphaBeta: {ab_win_rate1:.2f}%")
    
    if a_star_win_rate1 > a_star_win_rate2:
        print(f"  A* Bot å…ˆæ‰‹ä¼˜åŠ¿: {a_star_win_rate1 - a_star_win_rate2:.2f}ä¸ªç™¾åˆ†ç‚¹")
    if ab_win_rate2 > ab_win_rate1:
        print(f"  AlphaBeta Bot å…ˆæ‰‹ä¼˜åŠ¿: {ab_win_rate2 - ab_win_rate1:.2f}ä¸ªç™¾åˆ†ç‚¹")
    
    print("=" * 70)
    
    return {
        "scenario1": results1,
        "scenario2": results2,
        "total": {
            "A*_wins": total_a_star_wins,
            "AB_wins": total_ab_wins,
            "draws": total_draws,
            "total_trials": total_trials
        }
    }

# ==========================================
# VII. ä¸»ç¨‹åº
# ==========================================

if __name__ == "__main__":
    print("Robocon AI å¯¹æŠ—æµ‹è¯•ç³»ç»Ÿ")
    print("æ­£åœ¨è¿è¡Œ50æ¬¡å¯¹æŠ—æµ‹è¯•...\n")
    
    # è¿è¡Œ50æ¬¡å¯¹æŠ—
    results = run_batch_duels(num_trials=50)
    
    print("\næµ‹è¯•å®Œæˆï¼")
